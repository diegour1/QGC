{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7VwQfaCOcok"
      },
      "source": [
        "## Quantum Generative Classification with Mixed States on $28 \\times 28$ Binary (3, 6) MNIST. Training is performed first discriminative and then generative.\n",
        "\n",
        "Diego Useche Reyes"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GPU"
      ],
      "metadata": {
        "id": "Px6NXiW6Q74M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Al0mmMn7Q9oM",
        "outputId": "dc5428f2-f745-4717-e924-54e7ba774ec5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: nvidia-smi: command not found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HPeuojM4RUB1"
      },
      "source": [
        "## Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M5IafHoCOqgN",
        "outputId": "de8d8665-4a4c-4ee8-c56a-349977a5a032"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorcircuit\n",
            "  Downloading tensorcircuit-0.12.0-py3-none-any.whl.metadata (29 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from tensorcircuit) (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from tensorcircuit) (1.13.1)\n",
            "Collecting tensornetwork-ng (from tensorcircuit)\n",
            "  Downloading tensornetwork_ng-0.5.0-py3-none-any.whl.metadata (7.0 kB)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from tensorcircuit) (3.3)\n",
            "Requirement already satisfied: graphviz>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from tensornetwork-ng->tensorcircuit) (0.20.3)\n",
            "Requirement already satisfied: opt-einsum>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from tensornetwork-ng->tensorcircuit) (3.3.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensornetwork-ng->tensorcircuit) (3.11.0)\n",
            "Downloading tensorcircuit-0.12.0-py3-none-any.whl (342 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m342.0/342.0 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensornetwork_ng-0.5.0-py3-none-any.whl (243 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m243.3/243.3 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tensornetwork-ng, tensorcircuit\n",
            "Successfully installed tensorcircuit-0.12.0 tensornetwork-ng-0.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorcircuit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "15DqVUX2Ocop",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ccc81f51-a177-418b-8989-3e8e7a9a49be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorcircuit.translation:Please first ``pip install -U qiskit`` to enable related functionality in translation module\n",
            "WARNING:tensorcircuit.translation:Please first ``pip install -U cirq`` to enable related functionality in translation module\n"
          ]
        }
      ],
      "source": [
        "from functools import partial\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorcircuit as tc\n",
        "from tensorcircuit import keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yzQg29OoOcos",
        "outputId": "5f5028d9-04a5-4bb8-d52b-8c6b83baf5d6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('complex128', 'float64')"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "tc.set_backend(\"tensorflow\")\n",
        "tc.set_dtype(\"complex128\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P856730WaRBB",
        "outputId": "5c5c3557-b391-44c6-dbab-614e53c6688f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.17.0\n",
            "[LogicalDevice(name='/device:CPU:0', device_type='CPU')]\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras import layers, Model\n",
        "from tensorflow.keras.models import Sequential\n",
        "from keras.optimizers import SGD\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "print(tf.__version__)\n",
        "print(tf.config.list_logical_devices())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XHUQ385IUKo4"
      },
      "source": [
        "## Utils functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HcxPN_lIUNI0",
        "outputId": "67a58306-b598-4c4a-b1b9-a4bc739d51c9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 4, 8, 12]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "# this function takes the number of classes and of qubits of the qmc pure, and extract the indices\n",
        "# of the bit strings that correpond to the classes prediction\n",
        "## Example, qmc prediction bit string [\"00\", \"01\", \"10\", \"11\"]\n",
        "## the classes prediction is encoded in [\"00\", \"10\"], then it returns [0, 2]\n",
        "\n",
        "def _indices_qubits_classes(num_qubits_param, num_classes_param):\n",
        "  num_qubits_classes_temp = int(np.ceil(np.log2(num_classes_param)))\n",
        "  a = [np.binary_repr(i, num_qubits_param) for i in range(2**num_qubits_param)]\n",
        "  b = [(np.binary_repr(i, num_qubits_classes_temp) + \"0\"*(num_qubits_param - num_qubits_classes_temp)) for i in range(num_classes_param)]\n",
        "  indices_temp = []\n",
        "  for i in range(len(a)):\n",
        "    if a[i] in b:\n",
        "      indices_temp.append(i)\n",
        "\n",
        "  return indices_temp\n",
        "\n",
        "_indices_qubits_classes(4, 4)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MNIST Dataset"
      ],
      "metadata": {
        "id": "VRK7Lqt_pVVy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import collections\n",
        "\n",
        "\"\"\"\n",
        "Module for MNIST Digits Dataset preprocessing.\n",
        "https://www.tensorflow.org/quantum/tutorials/mnist\n",
        "\n",
        "Python 3.10.11\n",
        "\"\"\"\n",
        "\n",
        "def filter_by_classes(x, y, classes=[3,6]):\n",
        "    \"\"\"\n",
        "    Function that filters the MNIST Digits Dataset and returns samples on 'classes'.\n",
        "    Parameters:\n",
        "        x: Sample images.\n",
        "        y: Sample labels.\n",
        "        classes: List of classes to filter.\n",
        "    Returns:\n",
        "        x: x filtered by 'classes'.\n",
        "        y: x filtered by 'classes'.\n",
        "    \"\"\"\n",
        "    if not all(np.isin(classes, range(0, 10))):\n",
        "        return ValueError(\"Classes must be a list of digits (0-9).\")\n",
        "    x, y = x[np.isin(y, classes)], y[np.isin(y, classes)]\n",
        "    if len(classes)==2:\n",
        "        return x, y==classes[-1]\n",
        "    else:\n",
        "        return x, y\n",
        "\n",
        "def remove_contradicting(xs, ys):\n",
        "\n",
        "    mapping = collections.defaultdict(set)\n",
        "    orig_x = {}\n",
        "    # Determine the set of labels for each unique image:\n",
        "    for x,y in zip(xs,ys):\n",
        "       orig_x[tuple(x.flatten())] = x\n",
        "       mapping[tuple(x.flatten())].add(y)\n",
        "\n",
        "    new_x = []\n",
        "    new_y = []\n",
        "    for flatten_x in mapping:\n",
        "      x = orig_x[flatten_x]\n",
        "      labels = mapping[flatten_x]\n",
        "      if len(labels) == 1:\n",
        "          new_x.append(x)\n",
        "          new_y.append(next(iter(labels)))\n",
        "      else:\n",
        "          # Throw out images that match more than one label.\n",
        "          pass\n",
        "\n",
        "    num_uniq_3 = sum(1 for value in mapping.values() if len(value) == 1 and True in value)\n",
        "    num_uniq_6 = sum(1 for value in mapping.values() if len(value) == 1 and False in value)\n",
        "    num_uniq_both = sum(1 for value in mapping.values() if len(value) == 2)\n",
        "\n",
        "    print(\"Number of unique images:\", len(mapping.values()))\n",
        "    print(\"Number of unique 3s: \", num_uniq_3)\n",
        "    print(\"Number of unique 6s: \", num_uniq_6)\n",
        "    print(\"Number of unique contradicting labels (both 3 and 6): \", num_uniq_both)\n",
        "    print()\n",
        "    print(\"Initial number of images: \", len(xs))\n",
        "    print(\"Remaining non-contradicting unique images: \", len(new_x))\n",
        "\n",
        "    return np.array(new_x), np.array(new_y)\n",
        "\n",
        "def preprocess_mnist_digits(classes=[3,6]):\n",
        "    \"\"\"\"\n",
        "    Function that downloads the MNIST Digits dataset with TensorFlow and performs the following tasks:\n",
        "        1. Normalizes pixel values from (0, 255) to (0, 1).\n",
        "        2. By default, returns only 2 classes of digits for classification (this can be deactivated or modified by the 'classes' parameter).\n",
        "        3. Resizes samples to 4x4 images.\n",
        "        4. Removes samples that belong to multiple classes simultaneously.\n",
        "        5. Converts images to binary.\"\n",
        "    Parameters:\n",
        "    Returns:\n",
        "    \"\"\"\n",
        "\n",
        "    # Download dataset\n",
        "    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "    # Rescale the images from [0,255] to the [0.0,1.0] range.\n",
        "    x_train, x_test = x_train[..., np.newaxis]/255.0, x_test[..., np.newaxis]/255.0\n",
        "\n",
        "    # Filter to get only '3's and '6's\n",
        "    x_train, y_train = filter_by_classes(x_train, y_train, classes=classes)\n",
        "    x_test, y_test = filter_by_classes(x_test, y_test, classes=classes)\n",
        "\n",
        "    print(\"Number of filtered training examples:\", len(x_train))\n",
        "    print(\"Number of filtered test examples:\", len(x_test))\n",
        "\n",
        "    x_train_nocon, y_train_nocon = remove_contradicting(x_train, y_train)\n",
        "\n",
        "    THRESHOLD = 0.5\n",
        "\n",
        "    # Converts non contradicting samples to binary via threshold and converting bool to float.\n",
        "    x_train_bin = np.array(x_train_nocon > THRESHOLD, dtype=np.float32)\n",
        "    x_test_bin = np.array(x_test > THRESHOLD, dtype=np.float32)\n",
        "\n",
        "    return x_train_bin, y_train_nocon, x_test_bin, y_test\n",
        "\n",
        "X_train, y_train, X_test, y_test = preprocess_mnist_digits()\n",
        "X_train *= np.pi\n",
        "X_test *= np.pi\n",
        "y_train = np.where(y_train==False, 0, 1)\n",
        "y_test = np.where(y_test==False, 0, 1)\n",
        "y_train_oh = tf.reshape(tf.keras.backend.one_hot(y_train, 2), (-1,2))\n",
        "y_test_oh = tf.reshape(tf.keras.backend.one_hot(y_train, 2), (-1,2))\n",
        "\n",
        "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape, y_train_oh.shape, y_test_oh.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ntWxnOeg8PVy",
        "outputId": "c76a1323-2e8f-49cd-c2cd-eb480b57aa3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of filtered training examples: 12049\n",
            "Number of filtered test examples: 1968\n",
            "Number of unique images: 12049\n",
            "Number of unique 3s:  5918\n",
            "Number of unique 6s:  6131\n",
            "Number of unique contradicting labels (both 3 and 6):  0\n",
            "\n",
            "Initial number of images:  12049\n",
            "Remaining non-contradicting unique images:  12049\n",
            "(12049, 28, 28, 1) (12049,) (1968, 28, 28, 1) (1968,) (12049, 2) (12049, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 1: Mixed QMC variational, QEFF, APA, generative with Le-Net Conv layer. Tranining: first discriminative then generative."
      ],
      "metadata": {
        "id": "bRyLPoxw5PuN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Quantum variational KDC with QEFF\n",
        "\n",
        "import tensorcircuit as tc\n",
        "from tensorcircuit import keras\n",
        "import tensorflow as tf\n",
        "\n",
        "from functools import partial\n",
        "import numpy as np\n",
        "import math as m\n",
        "from scipy.stats import entropy, spearmanr\n",
        "\n",
        "tc.set_backend(\"tensorflow\")\n",
        "tc.set_dtype(\"complex128\")\n",
        "\n",
        "pi = tf.constant(m.pi)\n",
        "class VQKDC_MIXED_QEFF_LENET:\n",
        "    r\"\"\"\n",
        "    Defines the ready-to-use Quantum measurement classification (QMC) model implemented\n",
        "    in TensorCircuit using the TensorFlow/Keras API. Any additional argument in the methods has to be Keras-compliant.\n",
        "\n",
        "    Args:\n",
        "        auto_compile: A boolean to autocompile the model using default settings. (Default True).\n",
        "        var_pure_state_size:\n",
        "        gamma:\n",
        "\n",
        "    Returns:\n",
        "        An instantiated model ready to train with ad-hoc data.\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self, n_qeff_qubits, n_ancilla_qubits, num_classes_qubits, num_classes_param, dim_lenet_out_param, input_dim_param, gamma, n_training_data,  reduction = \"none\", training_type = \"generative\", batch_size = 16, learning_rate = 0.0005, random_state = 15, auto_compile=True):\n",
        "\n",
        "        self.circuit = None\n",
        "        self.gamma = gamma\n",
        "        self.num_classes = num_classes_param\n",
        "        self.num_classes_qubits = num_classes_qubits\n",
        "        self.n_qeff_qubits = n_qeff_qubits\n",
        "        self.n_ancilla_qubits = n_ancilla_qubits\n",
        "        self.n_total_qubits_temp = self.num_classes_qubits + self.n_qeff_qubits + self.n_ancilla_qubits\n",
        "        self.num_ffs = 2**self.n_qeff_qubits\n",
        "        self.n_training_data = n_training_data\n",
        "        self.dim_lenet_out = dim_lenet_out_param\n",
        "        self.input_dim = input_dim_param\n",
        "        self.var_pure_state_parameters_size = 2*(2**self.n_total_qubits_temp - 1)\n",
        "        self.reduction  = reduction\n",
        "        self.training_type = training_type\n",
        "        self.learning_rate = learning_rate\n",
        "        self.batch_size = batch_size\n",
        "        self.qeff_weights = tf.random.normal((self.dim_lenet_out, int(self.num_ffs*1-1)), mean = 0.0, stddev = 2.0/np.sqrt(self.num_ffs - 1), dtype=tf.dtypes.float64, seed = random_state)\n",
        "\n",
        "        layer = keras.QuantumLayer(\n",
        "            partial(self.layer),\n",
        "            [(self.var_pure_state_parameters_size,)]\n",
        "            )\n",
        "\n",
        "        ## build the Let-net model\n",
        "        self.model = Sequential()\n",
        "        self.model.add(tf.keras.layers.Conv2D(filters=20, kernel_size=(5, 5), padding=\"same\", activation='relu', input_shape=(self.input_dim, self.input_dim, 1)))\n",
        "        self.model.add(tf.keras.layers.AveragePooling2D(pool_size=(2, 2), strides=2))\n",
        "        self.model.add(tf.keras.layers.Conv2D(filters=50, kernel_size=(5, 5), padding=\"same\", activation='relu'))\n",
        "        self.model.add(tf.keras.layers.AveragePooling2D(pool_size=(2, 2), strides=2))\n",
        "        self.model.add(tf.keras.layers.Flatten())\n",
        "        self.model.add(tf.keras.layers.Dense(units=84, activation='relu'))\n",
        "        self.model.add(tf.keras.layers.Dense(units=self.dim_lenet_out, activation = None)) # original \"softmax\"\n",
        "\n",
        "        # add the quantum layer\n",
        "\n",
        "        self.model.add(layer)\n",
        "        print(self.model.summary())\n",
        "\n",
        "        if auto_compile:\n",
        "            self.compile()\n",
        "\n",
        "    def layer(\n",
        "            self,\n",
        "            x_sample_param,\n",
        "            var_pure_state_param,\n",
        "        ):\n",
        "        r\"\"\"\n",
        "        Defines a Density Matrix Kernel Density Estimation quantum layer for learning with fixed qaff (Meaning of qaff?). (This function was originally named dmkde_mixed_variational_density_estimation_fixed_qaff)\n",
        "\n",
        "        Args:\n",
        "            U_dagger:\n",
        "            var_pure_state_param:\n",
        "\n",
        "        Returns:\n",
        "            The probabilities of :math:`|k\\rangle`, `|1\\rangle`, ..., `|k\\rangle` state for kernel density classification of the classes.\n",
        "        \"\"\"\n",
        "\n",
        "        ### indices pure state\n",
        "        index_it = iter(np.arange(len(var_pure_state_param)))\n",
        "\n",
        "        ### indices qeff\n",
        "        index_iter_qeff = iter(np.arange(self.qeff_weights.shape[1]))\n",
        "\n",
        "        ### indices classes, of ms\n",
        "        n_qubits_classes_qeff_temp = self.num_classes_qubits + self.n_qeff_qubits\n",
        "        index_qubit_states = _indices_qubits_classes(n_qubits_classes_qeff_temp, self.num_classes) # extract indices of the bit string of classes\n",
        "\n",
        "\n",
        "        # Instantiate a circuit with the calculated number of qubits.\n",
        "        self.circuit = tc.Circuit(self.n_total_qubits_temp)\n",
        "\n",
        "        def circuit_base_ry_n(qc_param, num_qubits_param, target_qubit_param):\n",
        "            if num_qubits_param == 1:\n",
        "                qc_param.ry(0, theta = var_pure_state_param[next(index_it)])\n",
        "            elif num_qubits_param == 2:\n",
        "                qc_param.ry(target_qubit_param, theta=var_pure_state_param[next(index_it)])\n",
        "                qc_param.cnot(0, target_qubit_param)\n",
        "                qc_param.ry(target_qubit_param, theta=var_pure_state_param[next(index_it)])\n",
        "                return\n",
        "            else:\n",
        "                circuit_base_ry_n(qc_param, num_qubits_param-1, target_qubit_param)\n",
        "                qc_param.cnot(num_qubits_param-2, target_qubit_param)\n",
        "                circuit_base_ry_n(qc_param, num_qubits_param-1, target_qubit_param)\n",
        "                target_qubit_param -= 1\n",
        "\n",
        "        def circuit_base_rz_n(qc_param, num_qubits_param, target_qubit_param):\n",
        "            if num_qubits_param == 1:\n",
        "                qc_param.rz(0, theta = var_pure_state_param[next(index_it)])\n",
        "            elif num_qubits_param == 2:\n",
        "                qc_param.rz(target_qubit_param, theta=var_pure_state_param[next(index_it)])\n",
        "                qc_param.cnot(0, target_qubit_param)\n",
        "                qc_param.rz(target_qubit_param, theta=var_pure_state_param[next(index_it)])\n",
        "                return\n",
        "            else:\n",
        "                circuit_base_rz_n(qc_param, num_qubits_param-1, target_qubit_param)\n",
        "                qc_param.cnot(num_qubits_param-2, target_qubit_param)\n",
        "                circuit_base_rz_n(qc_param, num_qubits_param-1, target_qubit_param)\n",
        "                target_qubit_param -= 1\n",
        "\n",
        "        # Learning pure state\n",
        "        for i in range(1, self.n_total_qubits_temp+1):\n",
        "            circuit_base_ry_n(self.circuit, i, i-1)\n",
        "\n",
        "        # Learning pure state complex phase\n",
        "        for j in range(1, self.n_total_qubits_temp+1):\n",
        "            circuit_base_rz_n(self.circuit, j, j-1)\n",
        "\n",
        "        # Value to predict\n",
        "\n",
        "        x_sample_temp = tf.expand_dims(x_sample_param, axis=0)\n",
        "        phases_temp = (tf.cast(tf.sqrt(self.gamma), tf.float64)*tf.linalg.matmul(tf.cast(x_sample_temp, tf.float64), self.qeff_weights))[0]\n",
        "        init_qubit_qeff_temp = self.num_classes_qubits # qubit at which the qaff mapping starts it starts after the qubits of the classes\n",
        "\n",
        "        def circuit_base_rz_qeff_n(qc_param, num_qubits_param, target_qubit_param, init_qubit_param):\n",
        "          if num_qubits_param == 1:\n",
        "            qc_param.rz(init_qubit_param, theta = phases_temp[next(index_iter_qeff)] )\n",
        "          elif num_qubits_param == 2:\n",
        "            qc_param.rz(target_qubit_param + init_qubit_param, theta = phases_temp[next(index_iter_qeff)])\n",
        "            qc_param.cnot(init_qubit_param, target_qubit_param + init_qubit_param)\n",
        "            qc_param.rz(target_qubit_param + init_qubit_param, theta = phases_temp[next(index_iter_qeff)])\n",
        "            return\n",
        "          else:\n",
        "            circuit_base_rz_qeff_n(qc_param, num_qubits_param-1, target_qubit_param, init_qubit_param)\n",
        "            qc_param.cnot(num_qubits_param-2 + init_qubit_param, target_qubit_param + init_qubit_param)\n",
        "            circuit_base_rz_qeff_n(qc_param, num_qubits_param-1, target_qubit_param, init_qubit_param)\n",
        "            target_qubit_param -= 1\n",
        "\n",
        "        # Applying the QEFF feature map\n",
        "\n",
        "        for i in reversed(range(1, self.n_qeff_qubits + 1)):\n",
        "          circuit_base_rz_qeff_n(self.circuit, i, i - 1, init_qubit_qeff_temp)\n",
        "\n",
        "        for i in range(init_qubit_qeff_temp, init_qubit_qeff_temp + self.n_qeff_qubits):\n",
        "          self.circuit.H(i)\n",
        "\n",
        "        # Trace out ancilla qubits, find probability of [000] state for density estimation\n",
        "        measurement_state = tc.quantum.reduced_density_matrix(\n",
        "                        self.circuit.state(),\n",
        "                        cut=[m for m in range(n_qubits_classes_qeff_temp, self.n_total_qubits_temp)])\n",
        "        measurements_results = tc.backend.real(tf.stack([measurement_state[index_qubit_states[i], index_qubit_states[i]] for i in range(self.num_classes)]))\n",
        "        if self.training_type == \"discriminative\":\n",
        "          measurements_results = measurements_results / tf.reduce_sum(measurements_results, axis = -1)\n",
        "        print(self.training_type)\n",
        "        return measurements_results\n",
        "\n",
        "    def custom_categorical_crossentropy(self, y_true, y_pred):\n",
        "      ## code generated with the aid of chat gpt\n",
        "      \"\"\"\n",
        "      Compute the categorical cross-entropy loss with mean reduction.\n",
        "\n",
        "      Args:\n",
        "      y_true: Tensor of true labels, shape (batch_size, num_classes).\n",
        "      y_pred: Tensor of predicted probabilities, shape (batch_size, num_classes).\n",
        "\n",
        "      Returns:\n",
        "      Scalar tensor representing the mean loss over the batch.\n",
        "      \"\"\"\n",
        "      # Ensure predictions are clipped to avoid log(0)\n",
        "      epsilon_two = 1e-7  # small constant to avoid division by zero\n",
        "      y_pred = tf.clip_by_value(y_pred, epsilon_two, np.inf)  # clip values to avoid log(0) originaly 1.0 - epsilon\n",
        "\n",
        "      # Compute the categorical cross-entropy loss for each sample\n",
        "      loss = -tf.reduce_sum(y_true * tf.math.log(y_pred), axis=-1)\n",
        "\n",
        "      if self.reduction == \"none\":\n",
        "        return loss\n",
        "      elif self.reduction == \"mean\":\n",
        "        # Compute the mean loss over the batch\n",
        "        mean_loss = tf.reduce_mean(loss)\n",
        "        return mean_loss\n",
        "      elif self.reduction == \"sum\":\n",
        "        # Compute the sum loss over the batch\n",
        "        sum_loss = tf.reduce_sum(loss)\n",
        "        return sum_loss\n",
        "      else:\n",
        "        return loss\n",
        "\n",
        "    def compile(\n",
        "            self,\n",
        "            optimizer=tf.keras.optimizers.Adam,\n",
        "            **kwargs):\n",
        "        r\"\"\"\n",
        "        Method to compile the model.\n",
        "\n",
        "        Args:\n",
        "            optimizer:\n",
        "            **kwargs: Any additional argument.\n",
        "\n",
        "        Returns:\n",
        "            None.\n",
        "        \"\"\"\n",
        "        self.model.compile(\n",
        "            loss = self.custom_categorical_crossentropy,\n",
        "            optimizer=optimizer(self.learning_rate),\n",
        "            metrics=[\"accuracy\"],\n",
        "            **kwargs\n",
        "        )\n",
        "\n",
        "    def fit(self, x_train, y_train, batch_size=16, epochs = 30, **kwargs):\n",
        "        r\"\"\"\n",
        "        Method to fit (train) the model using the ad-hoc dataset.\n",
        "\n",
        "        Args:\n",
        "            x_train:\n",
        "            y_train:\n",
        "            batch_size:\n",
        "            epochs:\n",
        "            **kwargs: Any additional argument.\n",
        "\n",
        "        Returns:\n",
        "            None.\n",
        "        \"\"\"\n",
        "\n",
        "        self.model.fit(x_train, y_train, batch_size = self.batch_size, epochs = epochs, **kwargs)\n",
        "\n",
        "    def predict(self, x_test):\n",
        "      r\"\"\"\n",
        "      Method to make predictions with the trained model.\n",
        "\n",
        "      Args:\n",
        "          x_test:\n",
        "\n",
        "      Returns:\n",
        "          The predictions of the conditional density estimation of the input data.\n",
        "      \"\"\"\n",
        "      return (tf.experimental.numpy.power((self.gamma/(pi)), self.dim_lenet_out/2.)*\\\n",
        "          self.model.predict(x_test)).numpy()\n",
        "\n",
        "    def extract_lenet_features(self, x_test):\n",
        "      r\"\"\"\n",
        "      Method to extract the features of the Lenet.\n",
        "\n",
        "      Args:\n",
        "          x_test:\n",
        "\n",
        "      Returns:\n",
        "          Features of the Lenet.\n",
        "      \"\"\"\n",
        "      extract = Model(self.model.inputs, self.model.layers[-2].output)\n",
        "      new_model = Sequential()\n",
        "      new_model.add(extract)\n",
        "      return new_model.predict(x_test)\n",
        "\n",
        "    def freeze_lenet(self):\n",
        "      r\"\"\"\n",
        "      Method to frezze the layers of the Lenet.\n",
        "\n",
        "      Args:\n",
        "          self:\n",
        "\n",
        "      Returns:\n",
        "          None.\n",
        "      \"\"\"\n",
        "      for layer in self.model.layers[:-1]:\n",
        "          layer.trainable = False\n",
        "\n",
        "    def set_training_type(self, new_training_type):\n",
        "      r\"\"\"\n",
        "      Method to change the training type of the method during training.\n",
        "\n",
        "      Args:\n",
        "          new_training_strategy:\n",
        "\n",
        "      Returns:\n",
        "          None.\n",
        "      \"\"\"\n",
        "      self.training_type = new_training_type"
      ],
      "metadata": {
        "id": "UZDEPKZYFA1x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d911cb03-043e-47b0-8641-70a82d8d2f00",
        "id": "1o9jCTj_5PuQ"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5, 4, 1, 0.25, 123, 123, 0.0005, 12049, 'none', 'generative')"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "# Define a LeNet CNN feature extraction model\n",
        "input_shape = X_train.shape[1:]\n",
        "INPUT_DIM = input_shape[0]\n",
        "NUM_QUBITS_FFS = 4 ## originally N_QEFFS = 16\n",
        "NUM_CLASSES = 2\n",
        "NUM_CLASSES_QUBITS = 1\n",
        "DIM_LENET_OUT = 30 # originally 16\n",
        "GAMMA = 2**(-2) ## originally 2**(0)\n",
        "EPOCHS = 5\n",
        "N_TRAINING_DATA = X_train.shape[0]\n",
        "BATCH_SIZE = 32\n",
        "LEARNING_RATE = 0.0005 ## originally 0.0005\n",
        "RANDOM_STATE_QEFF = 123\n",
        "NUM_ANCILLA_QUBITS = 1\n",
        "REDUCTION = \"none\"\n",
        "TRAINING_TYPE = \"generative\"\n",
        "\n",
        "EPOCHS, NUM_QUBITS_FFS, NUM_ANCILLA_QUBITS, GAMMA, RANDOM_STATE_QEFF, RANDOM_STATE_QEFF, LEARNING_RATE, N_TRAINING_DATA, REDUCTION, TRAINING_TYPE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 643
        },
        "outputId": "f97879a6-e586-4db5-b4e3-04454f518d6e",
        "id": "S3I17P_w5PuQ"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "generative\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m20\u001b[0m)          │             \u001b[38;5;34m520\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ average_pooling2d_2                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m20\u001b[0m)          │               \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mAveragePooling2D\u001b[0m)                   │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv2d_3 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m50\u001b[0m)          │          \u001b[38;5;34m25,050\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ average_pooling2d_3                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m50\u001b[0m)            │               \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mAveragePooling2D\u001b[0m)                   │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ flatten_1 (\u001b[38;5;33mFlatten\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2450\u001b[0m)                │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m84\u001b[0m)                  │         \u001b[38;5;34m205,884\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m)                  │           \u001b[38;5;34m2,550\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ quantum_layer_1 (\u001b[38;5;33mQuantumLayer\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)                   │             \u001b[38;5;34m126\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">520</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ average_pooling2d_2                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)          │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">AveragePooling2D</span>)                   │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)          │          <span style=\"color: #00af00; text-decoration-color: #00af00\">25,050</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ average_pooling2d_3                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)            │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">AveragePooling2D</span>)                   │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ flatten_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2450</span>)                │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">84</span>)                  │         <span style=\"color: #00af00; text-decoration-color: #00af00\">205,884</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">2,550</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ quantum_layer_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">QuantumLayer</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">126</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m234,130\u001b[0m (915.06 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">234,130</span> (915.06 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m234,130\u001b[0m (915.06 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">234,130</span> (915.06 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n",
            "Epoch 1/5\n",
            "generative\n",
            "\u001b[1m377/377\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 98ms/step - accuracy: 0.7050 - loss: 1.3841\n",
            "Epoch 2/5\n",
            "\u001b[1m377/377\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 96ms/step - accuracy: 0.9982 - loss: 0.0308\n",
            "Epoch 3/5\n",
            "\u001b[1m377/377\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 79ms/step - accuracy: 0.9992 - loss: 0.0138\n",
            "Epoch 4/5\n",
            "\u001b[1m377/377\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 78ms/step - accuracy: 0.9997 - loss: 0.0099\n",
            "Epoch 5/5\n",
            "\u001b[1m377/377\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 79ms/step - accuracy: 0.9995 - loss: 0.0045\n"
          ]
        }
      ],
      "source": [
        "## this code creates a discriminative model\n",
        "vqkdc = VQKDC_MIXED_QEFF_LENET(n_qeff_qubits = NUM_QUBITS_FFS, n_ancilla_qubits =  NUM_ANCILLA_QUBITS, num_classes_qubits = NUM_CLASSES_QUBITS, num_classes_param = NUM_CLASSES, dim_lenet_out_param = DIM_LENET_OUT, input_dim_param = INPUT_DIM, gamma=GAMMA, n_training_data = N_TRAINING_DATA,  reduction = REDUCTION, training_type = TRAINING_TYPE, batch_size = BATCH_SIZE, learning_rate = LEARNING_RATE, random_state = RANDOM_STATE_QEFF, auto_compile = False)\n",
        "optimizer = tf.keras.optimizers.Adam(LEARNING_RATE)\n",
        "vqkdc.model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "vqkdc.fit(X_train, y_train_oh, epochs = EPOCHS)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# this code frezzes the weights of the Le net layer, and then sets the model to be trained in a generative way\n",
        "for layer in vqkdc.model.layers[:-1]:\n",
        "    layer.trainable = False\n",
        "vqkdc.compile()\n",
        "vqkdc.fit(X_train, y_train_oh, epochs = EPOCHS)"
      ],
      "metadata": {
        "id": "AAtO5lUhInuA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bdee4991-c6c4-43ed-9aa1-c64266e4abcc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m377/377\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 37ms/step - accuracy: 0.5797 - loss: 4.3470\n",
            "Epoch 2/5\n",
            "\u001b[1m377/377\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 36ms/step - accuracy: 0.5027 - loss: 3.0319\n",
            "Epoch 3/5\n",
            "\u001b[1m377/377\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 35ms/step - accuracy: 0.5107 - loss: 2.3592\n",
            "Epoch 4/5\n",
            "\u001b[1m377/377\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 37ms/step - accuracy: 0.5087 - loss: 1.9316\n",
            "Epoch 5/5\n",
            "\u001b[1m377/377\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 40ms/step - accuracy: 0.5048 - loss: 1.6590\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vqkdc.fit(X_train, y_train_oh, epochs = EPOCHS)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ixOW5peKyZzx",
        "outputId": "3c523f55-f4f6-448a-f863-844525512970"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m377/377\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 36ms/step - accuracy: 0.5034 - loss: 1.3823\n",
            "Epoch 2/5\n",
            "\u001b[1m377/377\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 36ms/step - accuracy: 0.5096 - loss: 1.1684\n",
            "Epoch 3/5\n",
            "\u001b[1m377/377\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 37ms/step - accuracy: 0.5024 - loss: 1.0728\n",
            "Epoch 4/5\n",
            "\u001b[1m377/377\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 37ms/step - accuracy: 0.4983 - loss: 0.9834\n",
            "Epoch 5/5\n",
            "\u001b[1m377/377\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 37ms/step - accuracy: 0.9865 - loss: 0.8967\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vqkdc.fit(X_train, y_train_oh, epochs = 2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tGi_8OmYy1VG",
        "outputId": "5d00b8b3-23fb-4fd2-992c-ad3dc7db823d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "\u001b[1m377/377\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 45ms/step - accuracy: 0.9996 - loss: 0.8509\n",
            "Epoch 2/2\n",
            "\u001b[1m377/377\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 37ms/step - accuracy: 0.9979 - loss: 0.8230\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = vqkdc.predict(X_test)\n",
        "\n",
        "accuracy_score(y_test, np.argmax(y_pred, axis=1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "taSERS2QNOTw",
        "outputId": "d5dead2b-d80c-458b-9e9d-93ee6abd195e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "generative\n",
            "\u001b[1m61/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 23ms/stepgenerative\n",
            "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 918ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9979674796747967"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 2: Mixed QMC variational, QEFF, HEA, generative with Le-Net Conv layer. Tranining: first discriminative then generative."
      ],
      "metadata": {
        "id": "Da8Z5rQvyQqg"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PjsJO7YGAxsN"
      },
      "source": [
        "The number of parameters of the Hardware efficient ansatz is given by,\n",
        "\n",
        "\n",
        "```\n",
        "HEA_size = n_qubits * (num_layers_hea + 1) * 2\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Quantum variational KDC with QEFF\n",
        "\n",
        "import tensorcircuit as tc\n",
        "from tensorcircuit import keras\n",
        "import tensorflow as tf\n",
        "\n",
        "from functools import partial\n",
        "import numpy as np\n",
        "import math as m\n",
        "from scipy.stats import entropy, spearmanr\n",
        "\n",
        "tc.set_backend(\"tensorflow\")\n",
        "tc.set_dtype(\"complex128\")\n",
        "\n",
        "pi = tf.constant(m.pi)\n",
        "\n",
        "class VQKDC_MIXED_QEFF_HEA_LENET:\n",
        "    r\"\"\"\n",
        "    Defines the ready-to-use Quantum measurement classification (QMC) model implemented\n",
        "    in TensorCircuit using the TensorFlow/Keras API. Any additional argument in the methods has to be Keras-compliant.\n",
        "\n",
        "    Args:\n",
        "        auto_compile: A boolean to autocompile the model using default settings. (Default True).\n",
        "        var_pure_state_size:\n",
        "        gamma:\n",
        "\n",
        "    Returns:\n",
        "        An instantiated model ready to train with ad-hoc data.\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self, n_qeff_qubits, n_ancilla_qubits, num_classes_qubits, num_classes_param, dim_lenet_out_param,  input_dim_param, gamma, n_training_data, reduction = \"none\", training_type = \"generative\", num_layers_hea = 3, batch_size = 16, learning_rate = 0.0005, random_state = 15, auto_compile=True):\n",
        "\n",
        "        self.circuit = None\n",
        "        self.gamma = gamma\n",
        "        self.num_layers_hea = num_layers_hea\n",
        "        self.num_classes = num_classes_param\n",
        "        self.num_classes_qubits = num_classes_qubits\n",
        "        self.n_qeff_qubits = n_qeff_qubits\n",
        "        self.n_ancilla_qubits = n_ancilla_qubits\n",
        "        self.n_total_qubits_temp = self.num_classes_qubits + self.n_qeff_qubits + self.n_ancilla_qubits\n",
        "        self.num_ffs = 2**self.n_qeff_qubits\n",
        "        self.n_training_data = n_training_data\n",
        "        self.dim_lenet_out = dim_lenet_out_param\n",
        "        self.input_dim = input_dim_param\n",
        "        self.var_hea_ansatz_size = int(self.n_total_qubits_temp*(self.num_layers_hea+1)*2)\n",
        "        self.reduction  = reduction\n",
        "        self.training_type = training_type\n",
        "        self.learning_rate = learning_rate\n",
        "        self.batch_size = batch_size\n",
        "        self.qeff_weights = tf.random.normal((self.dim_lenet_out, int(self.num_ffs*1-1)), mean = 0.0, stddev = 2.0/np.sqrt(self.num_ffs - 1), dtype=tf.dtypes.float64, seed = random_state)\n",
        "\n",
        "        layer = keras.QuantumLayer(\n",
        "            partial(self.layer),\n",
        "            [(self.var_hea_ansatz_size,)]\n",
        "            )\n",
        "\n",
        "        ## build the Let-net model\n",
        "        self.model = Sequential()\n",
        "        self.model.add(tf.keras.layers.Conv2D(filters=20, kernel_size=(5, 5), padding=\"same\", activation='relu', input_shape=(self.input_dim, self.input_dim, 1)))\n",
        "        self.model.add(tf.keras.layers.AveragePooling2D(pool_size=(2, 2), strides=2))\n",
        "        self.model.add(tf.keras.layers.Conv2D(filters=50, kernel_size=(5, 5), padding=\"same\", activation='relu'))\n",
        "        self.model.add(tf.keras.layers.AveragePooling2D(pool_size=(2, 2), strides=2))\n",
        "        self.model.add(tf.keras.layers.Flatten())\n",
        "        self.model.add(tf.keras.layers.Dense(units=84, activation='relu'))\n",
        "        self.model.add(tf.keras.layers.Dense(units=self.dim_lenet_out, activation = None)) # original \"softmax\"\n",
        "\n",
        "        # add the quantum layer\n",
        "\n",
        "        self.model.add(layer)\n",
        "        print(self.model.summary())\n",
        "\n",
        "        if auto_compile:\n",
        "            self.compile()\n",
        "\n",
        "    def layer(\n",
        "            self,\n",
        "            x_sample_param,\n",
        "            var_hea_ansatz_param,\n",
        "        ):\n",
        "        r\"\"\"\n",
        "        Defines a Density Matrix Kernel Density Estimation quantum layer for learning with fixed qaff (Meaning of qaff?). (This function was originally named dmkde_mixed_variational_density_estimation_fixed_qaff)\n",
        "\n",
        "        Args:\n",
        "            U_dagger:\n",
        "            var_pure_state_param:\n",
        "\n",
        "        Returns:\n",
        "            The probabilities of :math:`|k\\rangle`, `|1\\rangle`, ..., `|k\\rangle` state for kernel density classification of the classes.\n",
        "        \"\"\"\n",
        "\n",
        "        ### indices pure state hea\n",
        "        index_iter_hea  = iter(np.arange(len(var_hea_ansatz_param)))\n",
        "\n",
        "        ### indices qeff\n",
        "        index_iter_qeff = iter(np.arange(self.qeff_weights.shape[1]))\n",
        "\n",
        "        ### indices classes, of ms\n",
        "        n_qubits_classes_qeff_temp = self.num_classes_qubits + self.n_qeff_qubits\n",
        "        index_qubit_states = _indices_qubits_classes(n_qubits_classes_qeff_temp, self.num_classes) # extract indices of the bit string of classes\n",
        "\n",
        "\n",
        "        # Instantiate a circuit with the calculated number of qubits.\n",
        "        self.circuit = tc.Circuit(self.n_total_qubits_temp)\n",
        "\n",
        "        def hea_ansatz(qc_param, num_qubits_param, num_layers_param):\n",
        "          # encoding\n",
        "          for i in range (0, num_qubits_param):\n",
        "            qc_param.ry(i, theta = var_hea_ansatz_param[next(index_iter_hea)])\n",
        "            qc_param.rz(i, theta = var_hea_ansatz_param[next(index_iter_hea)])\n",
        "          # layers\n",
        "          for j in range(num_layers_param):\n",
        "            for i in range (0, num_qubits_param-1):\n",
        "              qc_param.CNOT(i, i+1)\n",
        "\n",
        "            for i in range (0, num_qubits_param):\n",
        "              qc_param.ry(i, theta= var_hea_ansatz_param[next(index_iter_hea)])\n",
        "              qc_param.rz(i, theta= var_hea_ansatz_param[next(index_iter_hea)])\n",
        "\n",
        "        ## learning pure state with HEA\n",
        "        hea_ansatz(self.circuit, self.n_total_qubits_temp, self.num_layers_hea)\n",
        "\n",
        "        # Value to predict\n",
        "\n",
        "        x_sample_temp = tf.expand_dims(x_sample_param, axis=0)\n",
        "        phases_temp = (tf.cast(tf.sqrt(self.gamma), tf.float64)*tf.linalg.matmul(tf.cast(x_sample_temp, tf.float64), self.qeff_weights))[0]\n",
        "        init_qubit_qeff_temp = self.num_classes_qubits # qubit at which the qaff mapping starts it starts after the qubits of the classes\n",
        "\n",
        "        def circuit_base_rz_qeff_n(qc_param, num_qubits_param, target_qubit_param, init_qubit_param):\n",
        "          if num_qubits_param == 1:\n",
        "            qc_param.rz(init_qubit_param, theta = phases_temp[next(index_iter_qeff)] )\n",
        "          elif num_qubits_param == 2:\n",
        "            qc_param.rz(target_qubit_param + init_qubit_param, theta = phases_temp[next(index_iter_qeff)])\n",
        "            qc_param.cnot(init_qubit_param, target_qubit_param + init_qubit_param)\n",
        "            qc_param.rz(target_qubit_param + init_qubit_param, theta = phases_temp[next(index_iter_qeff)])\n",
        "            return\n",
        "          else:\n",
        "            circuit_base_rz_qeff_n(qc_param, num_qubits_param-1, target_qubit_param, init_qubit_param)\n",
        "            qc_param.cnot(num_qubits_param-2 + init_qubit_param, target_qubit_param + init_qubit_param)\n",
        "            circuit_base_rz_qeff_n(qc_param, num_qubits_param-1, target_qubit_param, init_qubit_param)\n",
        "            target_qubit_param -= 1\n",
        "\n",
        "        # Applying the QEFF feature map\n",
        "\n",
        "        for i in reversed(range(1, self.n_qeff_qubits + 1)):\n",
        "          circuit_base_rz_qeff_n(self.circuit, i, i - 1, init_qubit_qeff_temp)\n",
        "\n",
        "        for i in range(init_qubit_qeff_temp, init_qubit_qeff_temp + self.n_qeff_qubits):\n",
        "          self.circuit.H(i)\n",
        "\n",
        "        # Trace out ancilla qubits, find probability of [000] state for density estimation\n",
        "        measurement_state = tc.quantum.reduced_density_matrix(\n",
        "                        self.circuit.state(),\n",
        "                        cut=[m for m in range(n_qubits_classes_qeff_temp, self.n_total_qubits_temp)])\n",
        "        measurements_results = tc.backend.real(tf.stack([measurement_state[index_qubit_states[i], index_qubit_states[i]] for i in range(self.num_classes)]))\n",
        "        if self.training_type == \"discriminative\":\n",
        "          measurements_results = measurements_results / tf.reduce_sum(measurements_results, axis = -1)\n",
        "        return measurements_results\n",
        "\n",
        "    def custom_categorical_crossentropy(self, y_true, y_pred):\n",
        "      ## code generated with the aid of chat gpt\n",
        "      \"\"\"\n",
        "      Compute the categorical cross-entropy loss with mean reduction.\n",
        "\n",
        "      Args:\n",
        "      y_true: Tensor of true labels, shape (batch_size, num_classes).\n",
        "      y_pred: Tensor of predicted probabilities, shape (batch_size, num_classes).\n",
        "\n",
        "      Returns:\n",
        "      Scalar tensor representing the mean loss over the batch.\n",
        "      \"\"\"\n",
        "      # Ensure predictions are clipped to avoid log(0)\n",
        "      epsilon_two = 1e-7  # small constant to avoid division by zero\n",
        "      y_pred = tf.clip_by_value(y_pred, epsilon_two, np.inf)  # clip values to avoid log(0) originaly 1.0 - epsilon\n",
        "\n",
        "      # Compute the categorical cross-entropy loss for each sample\n",
        "      loss = -tf.reduce_sum(y_true * tf.math.log(y_pred), axis=-1)\n",
        "\n",
        "      if self.reduction == \"none\":\n",
        "        return loss\n",
        "      elif self.reduction == \"mean\":\n",
        "        # Compute the mean loss over the batch\n",
        "        mean_loss = tf.reduce_mean(loss)\n",
        "        return mean_loss\n",
        "      elif self.reduction == \"sum\":\n",
        "        # Compute the sum loss over the batch\n",
        "        sum_loss = tf.reduce_sum(loss)\n",
        "        return sum_loss\n",
        "      else:\n",
        "        return loss\n",
        "\n",
        "    def compile(\n",
        "            self,\n",
        "            optimizer=tf.keras.optimizers.Adam,\n",
        "            **kwargs):\n",
        "        r\"\"\"\n",
        "        Method to compile the model.\n",
        "\n",
        "        Args:\n",
        "            optimizer:\n",
        "            **kwargs: Any additional argument.\n",
        "\n",
        "        Returns:\n",
        "            None.\n",
        "        \"\"\"\n",
        "        self.model.compile(\n",
        "            loss = self.custom_categorical_crossentropy,\n",
        "            optimizer=optimizer(self.learning_rate),\n",
        "            metrics=[\"accuracy\"],\n",
        "            **kwargs\n",
        "        )\n",
        "\n",
        "    def fit(self, x_train, y_train, batch_size=16, epochs = 30, **kwargs):\n",
        "        r\"\"\"\n",
        "        Method to fit (train) the model using the ad-hoc dataset.\n",
        "\n",
        "        Args:\n",
        "            x_train:\n",
        "            y_train:\n",
        "            batch_size:\n",
        "            epochs:\n",
        "            **kwargs: Any additional argument.\n",
        "\n",
        "        Returns:\n",
        "            None.\n",
        "        \"\"\"\n",
        "\n",
        "        self.model.fit(x_train, y_train, batch_size = self.batch_size, epochs = epochs, **kwargs)\n",
        "\n",
        "    def predict(self, x_test):\n",
        "      r\"\"\"\n",
        "      Method to make predictions with the trained model.\n",
        "\n",
        "      Args:\n",
        "          x_test:\n",
        "\n",
        "      Returns:\n",
        "          The predictions of the conditional density estimation of the input data.\n",
        "      \"\"\"\n",
        "      return (tf.experimental.numpy.power((self.gamma/(pi)), self.dim_lenet_out/2.)*\\\n",
        "          self.model.predict(x_test)).numpy()\n",
        "\n",
        "    def extract_lenet_features(self, x_test):\n",
        "      r\"\"\"\n",
        "      Method to extract the features of the Lenet.\n",
        "\n",
        "      Args:\n",
        "          x_test:\n",
        "\n",
        "      Returns:\n",
        "          Features of the Lenet.\n",
        "      \"\"\"\n",
        "      extract = Model(self.model.inputs, self.model.layers[-2].output)\n",
        "      new_model = Sequential()\n",
        "      new_model.add(extract)\n",
        "      return new_model.predict(x_test)\n",
        "\n",
        "    def freeze_lenet(self):\n",
        "      r\"\"\"\n",
        "      Method to frezze the layers of the Lenet.\n",
        "\n",
        "      Args:\n",
        "          self:\n",
        "\n",
        "      Returns:\n",
        "          None.\n",
        "      \"\"\"\n",
        "      for layer in self.model.layers[:-1]:\n",
        "          layer.trainable = False\n",
        "\n",
        "    def set_training_type(self, new_training_type):\n",
        "      r\"\"\"\n",
        "      Method to change the training type of the method during training.\n",
        "\n",
        "      Args:\n",
        "          new_training_strategy:\n",
        "\n",
        "      Returns:\n",
        "          None.\n",
        "      \"\"\"\n",
        "      self.training_type = new_training_type"
      ],
      "metadata": {
        "id": "XZik7fN8AxsO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0af45db0-3d1e-49aa-ea9e-fd3609079017",
        "id": "PZv34fX96m1y"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4, 4, 1, 0.25, 123, 123, 0.0005, 12049, 'none', 'generative')"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "# Define a LeNet CNN feature extraction model\n",
        "input_shape = X_train.shape[1:]\n",
        "INPUT_DIM = input_shape[0]\n",
        "NUM_QUBITS_FFS = 4 ## originally N_QEFFS = 16\n",
        "NUM_CLASSES = 2\n",
        "NUM_CLASSES_QUBITS = 1\n",
        "NUM_TOTAL_QUBITS = NUM_QUBITS_FFS + NUM_ANCILLA_QUBITS + NUM_CLASSES_QUBITS\n",
        "NUM_LAYERS_HEA = int(np.round(((2**NUM_TOTAL_QUBITS-1)/NUM_TOTAL_QUBITS)-1))\n",
        "DIM_LENET_OUT = 30 # originally 16\n",
        "GAMMA = 2**(-2) ## originally 2**(0)\n",
        "EPOCHS = 4 ## originally 8\n",
        "N_TRAINING_DATA = X_train.shape[0]\n",
        "BATCH_SIZE = 32\n",
        "LEARNING_RATE = 0.0005 ## originally 0.0005\n",
        "RANDOM_STATE_QEFF = 123\n",
        "NUM_ANCILLA_QUBITS = 1\n",
        "REDUCTION = \"none\"\n",
        "TRAINING_TYPE = \"generative\"\n",
        "\n",
        "EPOCHS, NUM_QUBITS_FFS, NUM_ANCILLA_QUBITS, GAMMA, RANDOM_STATE_QEFF, RANDOM_STATE_QEFF, LEARNING_RATE, N_TRAINING_DATA, REDUCTION, TRAINING_TYPE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 573
        },
        "outputId": "dfcc49b1-e5d1-473a-905c-674520ac8475",
        "id": "pwABUVk36tTP"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_3\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_3\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ conv2d_6 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m20\u001b[0m)          │             \u001b[38;5;34m520\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ average_pooling2d_6                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m20\u001b[0m)          │               \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mAveragePooling2D\u001b[0m)                   │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv2d_7 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m50\u001b[0m)          │          \u001b[38;5;34m25,050\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ average_pooling2d_7                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m50\u001b[0m)            │               \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mAveragePooling2D\u001b[0m)                   │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ flatten_3 (\u001b[38;5;33mFlatten\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2450\u001b[0m)                │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m84\u001b[0m)                  │         \u001b[38;5;34m205,884\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_7 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m)                  │           \u001b[38;5;34m2,550\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ quantum_layer_3 (\u001b[38;5;33mQuantumLayer\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)                   │             \u001b[38;5;34m132\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ conv2d_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">520</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ average_pooling2d_6                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)          │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">AveragePooling2D</span>)                   │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv2d_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)          │          <span style=\"color: #00af00; text-decoration-color: #00af00\">25,050</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ average_pooling2d_7                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)            │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">AveragePooling2D</span>)                   │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ flatten_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2450</span>)                │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">84</span>)                  │         <span style=\"color: #00af00; text-decoration-color: #00af00\">205,884</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">2,550</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ quantum_layer_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">QuantumLayer</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">132</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m234,136\u001b[0m (915.11 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">234,136</span> (915.11 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m234,136\u001b[0m (915.11 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">234,136</span> (915.11 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n",
            "Epoch 1/4\n",
            "\u001b[1m377/377\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 79ms/step - accuracy: 0.8607 - loss: 0.2689\n",
            "Epoch 2/4\n",
            "\u001b[1m377/377\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 79ms/step - accuracy: 0.9989 - loss: 0.0167\n",
            "Epoch 3/4\n",
            "\u001b[1m377/377\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 80ms/step - accuracy: 1.0000 - loss: 0.0016\n",
            "Epoch 4/4\n",
            "\u001b[1m377/377\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 90ms/step - accuracy: 1.0000 - loss: 0.0011\n"
          ]
        }
      ],
      "source": [
        "## this code creates a discriminative model\n",
        "vqkdc_hea = VQKDC_MIXED_QEFF_HEA_LENET(n_qeff_qubits = NUM_QUBITS_FFS, n_ancilla_qubits =  NUM_ANCILLA_QUBITS, num_classes_qubits = NUM_CLASSES_QUBITS, num_classes_param = NUM_CLASSES, dim_lenet_out_param = DIM_LENET_OUT, input_dim_param = INPUT_DIM, gamma=GAMMA, n_training_data = N_TRAINING_DATA,  reduction = REDUCTION, training_type = TRAINING_TYPE, num_layers_hea = NUM_LAYERS_HEA, batch_size = BATCH_SIZE, learning_rate = LEARNING_RATE, random_state = RANDOM_STATE_QEFF, auto_compile = False)\n",
        "optimizer = tf.keras.optimizers.Adam(LEARNING_RATE)\n",
        "vqkdc_hea.model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "vqkdc_hea.fit(X_train, y_train_oh, epochs = EPOCHS)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# this code frezzes the weights of the Le net layer, and then sets the model to be trained in a generative way\n",
        "for layer in vqkdc_hea.model.layers[:-1]:\n",
        "    layer.trainable = False\n",
        "vqkdc_hea.compile()\n",
        "vqkdc_hea.fit(X_train, y_train_oh, epochs = EPOCHS)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "57VK1cEPStNd",
        "outputId": "dd123c54-c57c-41c9-953c-fe8ef0188d70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/4\n",
            "\u001b[1m377/377\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 38ms/step - accuracy: 0.9992 - loss: 2.5462\n",
            "Epoch 2/4\n",
            "\u001b[1m377/377\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 38ms/step - accuracy: 0.9970 - loss: 1.2786\n",
            "Epoch 3/4\n",
            "\u001b[1m377/377\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 39ms/step - accuracy: 0.9964 - loss: 1.0120\n",
            "Epoch 4/4\n",
            "\u001b[1m377/377\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 38ms/step - accuracy: 0.9975 - loss: 0.9136\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vqkdc_hea.fit(X_train, y_train_oh, epochs = EPOCHS)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QaWbSgd61TWs",
        "outputId": "a43a9fcb-929c-489d-b4b0-6394de926666"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/4\n",
            "\u001b[1m377/377\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 55ms/step - accuracy: 0.9985 - loss: 0.8621\n",
            "Epoch 2/4\n",
            "\u001b[1m377/377\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 35ms/step - accuracy: 0.9982 - loss: 0.8350\n",
            "Epoch 3/4\n",
            "\u001b[1m377/377\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 36ms/step - accuracy: 0.9981 - loss: 0.8190\n",
            "Epoch 4/4\n",
            "\u001b[1m377/377\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 36ms/step - accuracy: 0.9971 - loss: 0.8141\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = vqkdc_hea.predict(X_test)\n",
        "\n",
        "accuracy_score(y_test, np.argmax(y_pred, axis=1))"
      ],
      "metadata": {
        "id": "h43owVQJAxsQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02831a07-0695-4fa0-8057-35bbe422a7fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 616ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9984756097560976"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}